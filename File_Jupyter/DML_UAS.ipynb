{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1b686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reference Classifier for Make Data Count Competition\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "245c75be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# === Step 1: Load Dataset ===\n",
    "data_dir = Path(\"E:/UMB/Semester 6/DML/UAS/Dataset\")\n",
    "train_labels = pd.read_csv(data_dir / \"train_labels.csv\")\n",
    "sample_submission = pd.read_csv(data_dir / \"sample_submission.csv\")\n",
    "train_xml_dir = data_dir / \"train/XML\"\n",
    "test_xml_dir = data_dir / \"test/XML\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74f8692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ekstrak teks dari XML\n",
    "def extract_text_from_xml(xml_path):\n",
    "    with open(xml_path, 'r', encoding='utf-8') as f:\n",
    "        soup = BeautifulSoup(f, 'xml')\n",
    "    body = soup.find('body')\n",
    "    return body.get_text(separator=' ', strip=True) if body else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de82b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing teks dasar\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9 ]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad29ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bangun dataset berdasarkan article_id dan label type\n",
    "samples = []\n",
    "for _, row in train_labels.iterrows():\n",
    "    article_id = row['article_id']\n",
    "    label = row['type'].lower()\n",
    "    if label not in ['primary', 'secondary']:\n",
    "        continue  # abaikan Missing\n",
    "    xml_file = train_xml_dir / f\"{article_id}.xml\"\n",
    "    if not xml_file.exists():\n",
    "        continue\n",
    "    text = extract_text_from_xml(xml_file)\n",
    "    text = preprocess(text)\n",
    "    samples.append((text, label))\n",
    "\n",
    "train_df = pd.DataFrame(samples, columns=['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0581e6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     primary       0.84      0.93      0.88        44\n",
      "   secondary       0.96      0.91      0.93        85\n",
      "\n",
      "    accuracy                           0.91       129\n",
      "   macro avg       0.90      0.92      0.91       129\n",
      "weighted avg       0.92      0.91      0.92       129\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['E:\\\\UMB\\\\Semester 6\\\\DML\\\\UAS\\\\Dataset\\\\model.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train-test split dan pipeline TFIDF + Logistic Regression\n",
    "X = train_df['text']\n",
    "y = train_df['label']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "joblib.dump(model, data_dir / \"model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d0c03dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === Step 6: Inference on Test Set ===\u001b[39;00m\n\u001b[32m      2\u001b[39m test_predictions = []\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel.joblib\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m xml_file \u001b[38;5;129;01min\u001b[39;00m test_xml_dir.glob(\u001b[33m\"\u001b[39m\u001b[33m*.xml\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      6\u001b[39m     doc_id = xml_file.stem\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\UMB\\Semester 6\\DML\\UAS\\DML_Kelompok-B\\.venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:735\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    733\u001b[39m         obj = _unpickle(fobj, ensure_native_byte_order=ensure_native_byte_order)\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _validate_fileobject_and_memmap(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[32m    737\u001b[39m             fobj,\n\u001b[32m    738\u001b[39m             validated_mmap_mode,\n\u001b[32m    739\u001b[39m         ):\n\u001b[32m    740\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    741\u001b[39m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[32m    742\u001b[39m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[32m    743\u001b[39m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'model.joblib'"
     ]
    }
   ],
   "source": [
    "# === Step 6: Inference on Test Set ===\n",
    "test_predictions = []\n",
    "model = joblib.load(\"model.joblib\")\n",
    "\n",
    "for xml_file in test_xml_dir.glob(\"*.xml\"):\n",
    "    doc_id = xml_file.stem\n",
    "    full_text = extract_text_from_xml(xml_file)\n",
    "    # Dummy chunking: sliding window (can be improved)\n",
    "    chunk_size = 30\n",
    "    stride = 15\n",
    "    predictions = []\n",
    "    for i in range(0, len(full_text) - chunk_size, stride):\n",
    "        chunk = full_text[i:i+chunk_size]\n",
    "        preprocessed = preprocess(chunk)\n",
    "        if not preprocessed.strip():\n",
    "            continue\n",
    "        pred = model.predict([preprocessed])[0]\n",
    "        predictions.append(f\"{pred} {i} {i+chunk_size}\")\n",
    "    prediction_string = \"|\".join(predictions)\n",
    "    test_predictions.append((doc_id, prediction_string))\n",
    "\n",
    "submission_df = pd.DataFrame(test_predictions, columns=['Id', 'PredictionString'])\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission saved as submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
